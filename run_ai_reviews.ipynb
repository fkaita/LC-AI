{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run AI reviewer for application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI API with model gpt-4o-2024-05-13.\n"
     ]
    }
   ],
   "source": [
    "# This is test code for comparing logic. I compared two methods for the task. \n",
    "# In the first baseline method, the AI (gpt-4o-2024-05-13) accepts the full texts of the L/C application and the contract, then identifies any discrepancies. \n",
    "# In the second method, the AI first splits the L/C application into parts and then checks each part for discrepancies against the contract.\n",
    "# The second method is used in the code.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "from llm import create_client, get_response_from_llm\n",
    "from review import extract_pdf_text\n",
    "\n",
    "# Create a real client\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client, model = create_client(\"gpt-4o-2024-05-13\")\n",
    "\n",
    "file_path = \"docs/application/002\" # Sample with mistakes in L/C application\n",
    "\n",
    "it_reviewer_system_prompt = (\n",
    "    \"You are an AI reviewer in the international trading company responsible for evaluating Letter of Credit (L/C) applications. \"\n",
    "    \"Your mission is to assess if all contents described in L/C application are align with the contract. \"\n",
    "    \"Be critical and cautious in your decision-making process. Your evaluations should be structured, concise, and accurate.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Simple Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_text = extract_pdf_text(file_path + \"/contract.pdf\")\n",
    "application_text = extract_pdf_text(file_path + \"/application.pdf\")\n",
    "\n",
    "base_prompt = f\"\"\"\n",
    "Below is a L/C application. Please carefully read the content and advice if any discrepancy exists with given contract.\n",
    "Please only provide identified discrepancy in your response.\n",
    "If you do not find any discrepancy, just mention no discrepancy.\n",
    "\n",
    "L/C APPLICATION:\n",
    "{application_text}\n",
    "\n",
    "PROVIDED CONTRACT:\n",
    "{contract_text}\n",
    "\"\"\"\n",
    "\n",
    "llm_review, msg_history = get_response_from_llm(\n",
    "    base_prompt,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    system_message=it_reviewer_system_prompt,\n",
    "    print_debug=False,\n",
    ")\n",
    "\n",
    "print(llm_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Split, Compare and Summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "base_prompt = \"\"\"\n",
    "Please split following document into multiple parts by content.\n",
    "*Please just split the original text without changing any words even if original text contains typo or mistake.\n",
    "*Please split without overlapping\n",
    "*Please provide output in JSON.\n",
    "\n",
    "Args: \n",
    "    application text (str): A texts from document \n",
    "\n",
    "Returns:\n",
    "    dict: A list of splitted text.\n",
    "\n",
    "    output format should be as below:\n",
    "    * Please only provide JSON output as string starting from { and ending with }\n",
    "\n",
    "    {\n",
    "        \"output\": [\"list of splitted texts\", \"list of splitted texts\", \"list of splitted texts\"...]\n",
    "        ...\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "application_text = extract_pdf_text(file_path + \"/application.pdf\")\n",
    "base_prompt += f\"Document: {application_text}\"\n",
    "\n",
    "llm_review, msg_history = get_response_from_llm(\n",
    "    base_prompt,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    system_message=it_reviewer_system_prompt,\n",
    "    print_debug=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    json_match = re.search(r\"{.*}\", llm_review, re.DOTALL)\n",
    "    if json_match:\n",
    "        cleaned_review = json_match.group(0)\n",
    "        review = json.loads(cleaned_review)\n",
    "    else:\n",
    "        raise ValueError(\"No JSON found in response\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decoding failed: {e}\")\n",
    "    print(f\"LLM response: {llm_review}\")\n",
    "    review = None\n",
    "\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_text = extract_pdf_text(file_path + \"/contract.pdf\")\n",
    "result = \"\"\n",
    "for text in review[\"output\"]:\n",
    "    base_prompt = f\"\"\"\n",
    "    Below paragraph is extracted from a L/C application. Please carefully read the content and advice if any discrepancy exists with given contract.\n",
    "    Please only provide identified discrepancy in your response.\n",
    "    If you do not find any discrepancy, just mention no discrepancy.\n",
    "\n",
    "    EXTRACTED PARAGRAPH FROM L/C APPLICATION:\n",
    "    {text}\n",
    "\n",
    "    PROVIDED CONTRACT:\n",
    "    {contract_text}\n",
    "    \"\"\"\n",
    "\n",
    "    llm_review, msg_history = get_response_from_llm(\n",
    "        base_prompt,\n",
    "        model=model,\n",
    "        client=client,\n",
    "        system_message=it_reviewer_system_prompt,\n",
    "        print_debug=False,\n",
    "    )\n",
    "\n",
    "    result += llm_review\n",
    "\n",
    "print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"\"\"\n",
    "    Please summarize the discrepancy found in the the below report.\n",
    "\n",
    "    Report:\n",
    "    {result}\n",
    "    \"\"\"\n",
    "\n",
    "llm_review, msg_history = get_response_from_llm(\n",
    "    base_prompt,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    system_message=it_reviewer_system_prompt,\n",
    "    print_debug=False,\n",
    ")\n",
    "\n",
    "print(llm_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sakana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
